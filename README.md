# Compiler Optimizing via Deep Reinforcement Learning (COREL)


## Overview

This work aims to employee Deep Reinforcement Learning to solve Compiler Optimizing. 

### Setup

To run this model clone the repo, create a new virtualenv, install requirements from requirements.txt, enjoy!

### Running the model

To run the model, after you've activated the virtualenv, run `$ python main.py`

### Visualizing the Results

To startup the interactive dashboard, run `$ python visualize.py`. 

### Generating Documentation

Documentation for the entire package can be generated by executing `make` from withing the `docs/` directory. To view the resulting docs locally, simply open `index.html` in your favorite browser.

## Release Notes

### Version 1.3

- Agents
    - New additions: GAN, GCN, CGCN
    - Save now uses the improved `keras.save()` method, rather than saving and loading weights. This means that parameters, and hyper-parameters are all stored and loaded automatically.
    - Documentation has been updated substantially to better describe each agent.
    - An abstract Agent has been built, normalizing the API between all available agents.
- Programs
    - Better protections have been put in place to ensure that if `USE_RUNTIMES` is `True`, no runtimes will be generated.
    - Documentation has been updated substantially to better describe the various methods and attributes of both individual Program(s) and the Programs container itself.
- Benchmarks
    - `Benchmark` is now abstract, as it should have always been. 
    - `cBench` is now able to load more data from cache, including entire runtime arrays for every program.

### Version 1.2

- Created a dynamic visualization dashboard capable of displaying key metrics for any given run.
- Enhanced runtime metric gathering to better diagnose odd performance curves.
- Expanded main training loop functionality via global constants in the settings file, new settings are:
	- `USE_RUNTIMES` controls whether or no each program uses it's pre-generated runtime data, rather than running each combination of compilation flags again.
	- `THREADED_RUNTIMES` controls whether or not threads are used to gather runtimes. Enabling this speeds up runtime gathering by as much as 4x, yet skews the resulting metrics. Future work will try to establish whether or not this has a material impact in the outcome. 
- Most classes now use Python's dunder methods for a more intuitive script. This helped readability throughout the project.
- Programs are now saved as JSON files, rather than Pickle files. This allows easier analytics, and closed a security hole in the program (minor as it may be).
- Programs will never return a 0 runtime. This prevents DivisionByZero errors for programs that simply run faster than `time`'s precision.
- Programs have a few more properties, namely o2, o3, and optimal. These properties keep track of program performance with the respective optimization flags; and the optimal performance found.
- The full run directory structure is created upon start of the main loop, preventing visualization errors due to missing files or directories. The files are then overwritten or appended to when necessary.
- Metrics are no longer written to a .log file, rather they are now written to a .csv, making read/write a bit easier to deal with.
- Benchmarks have a standardized API they must conform to, cBench has been brought into alignment. This base API that all benchmarks must inherit from provides some nice added functionality, including list(), iter(), and selection via [].
- Sphinx Documentation is now supported. If you'd like to read the source code in a more civilized manner, you may now run sphinx documentation generation, and review the resulting index.html file. Long term goal is to publish these documents when the project goes public.

### Version: 1.1

Reorganized logging structure in anticipation of a more robust visualization experience. 

### Version: 1.0

Simpler internal APIs, more intuitive classes, and simpler code.

### Version: 0.5

The model is officially ready for baseline testing! 

Ready to gather information on each feature set, with one being left out at each interval.

#### Programs

- Each program now contains all 3 feature-sets: static, dynamic, and hybrid. 

#### Main

- Main now dynamically determines the number of features in each set, training agents on those specific features.

### Version 0.4

This version has implemented metric gathering, and agent serialization via pickle.

#### Agents

- Can now be serialized via pickle! 

    - Model weights are not pickle-able, nor is the Callbacks used to log data to `Tensorboard`. Therefore, in order to make `Agents` pickle-able, I had to properly implement `__getstate__` and `__setstate__` to handle loading and saving model weights, and model construction. 

- Train and Test on each program available

    - A separate agent is now created for every program in the `Programs` class. This allows leave-one-out testing to be performed on all programs automatically. The agents take turns, getting `EPISODES` tries to compile and run the various programs in the training data. After `EPISODES` tries, the agent then sets it's `epsilon` to 0, and tries to achieve the lowest runtime on the left-out benchmark.

- Perform n-shot testing

    - `Agents` are now capable of producing n-actions, in sorted order of belief. This allows n-shot testing to occur. 

> The `Programs` class has been modified to account for this change

- Lost save and load methods

    - These methods have been removed, `main()` now takes care of loading the proper `agent`.

- Track their own steps

    - Tracking the amount of steps an `agent` has taken was becoming tedious in `main()`. The variable is now maintained by each `agent`.

