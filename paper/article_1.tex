\documentclass{article}
\title{Compiler Optimization using Generative Adversarial Networks}
\author{
    Thomas Howard III  \\
    University of Rhode Island  \\
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Compiling source code is an extremely complex process. In order to get the best performance, a developer must understand how dozens of individual 'optimization flags' interact with each other, and the source code itself, \textit{in sequence}. Since optimizations are applied to intermediate representations of code iteratively, and since each application of an optimization actually changes the code, relationships between flags and a faster runtime are extremely difficult to reason about. This work endeavours to show that deep learning, and specifically, Generative Adversarial Networks, are able to grasp this relationship, and use it to produce code that is extremely close to 'optimal', in terms of runtime.
\end{abstract}

\section{Introduction}
When compiling source code for release, most developers want the fastest runtime, most power efficient operation, and/or smallest memory footprint; yet, few actually know how to get this top-level performance out of modern compilers. In fact, GCC alone supports dozens of optimization flags that can be rearranged, repeated, left out completely, or any combination of the former. It is because of this extreme complexity that no perfect compiler optimization-sequence exists. Current solutions to this problem use various statistical and machine-learning-based methods to try and derive the best possible options, short-circuiting an essentially iterative processes. However, all current solutions fall short of truly optimal performance, or lack the ability to generalize across multiple architectures and benchmarks.

At this point, one may wonder if all of this effort is worth the reward - is a faster runtime that important? In short, yes. Consider embedded devices, such as cars, where code may be compiled only a single time, before being deployed to millions of devices. At this scale, even small gains add up quite quickly, however, through this work, we have found some standard benchmarks can be compiled to run upwards of 3x faster than with standard optimization levels. Imagine if all of the devices in your possession suddenly became 3x faster overnight, the result of such a speedup would be staggering, now multiply that feeling across every person on the planet.

\section{Previous Works}
Previous studies in compiler optimizations can be broadly classified into two distinct categories: \textit{selecting the best optimizations}, and \textit{phase ordering}. The work presented here is focused exclusively on the former, since the order of flags used was fixed; the later category attempts to solve for the best ordering of flags, and is something that may be studied in future works. 

\section{Training and Testing}
In order to thoroughly evaluate the proposed model, this work employed leave-one-out testing against each program, for each feature set present in the data. Training begins by first loading all cBench programs, their static and dynamic features, along with all 128 possible runtimes \footnote{Courtesy of the work done for COBAYN by Amir Ashouri}. Once all of this data is loaded into memory, a Generative Adversarial Network is constructed and trained on all programs except for the one being tested on.

The outer loop of the script cycles through each feature set: static, dynamic, and hybrid; while the inner loop cycles through each individual application. Due to the nature of leave-one-out testing, within the inner loop, a unique agent is created as a combination of the program to test against, and the feature set in use. Once this agent is instantiated, it is given 100 epochs to learn the relation between program features, compiler flag sequences, and runtimes. During each epoch the model is presented with vectors of features and noise; from which the generator tries to produce the 'optimal' flags. The discriminator then tries to determine whether or not these flags are indeed 'optimal'. These two networks are trained in an alternating pattern, with the discriminator receiving both fake flags from the generator, as well as true optimal combinations from the pre-built dataset.

\section{Results}


\section{Conclusions}



\begin{thebibliography}{9}
\bibitem[Ashouri]{ashouri} \emph{COBAYN: Compiler autotuning with BAYsian Network}
\end{thebibliography}

\end{document}